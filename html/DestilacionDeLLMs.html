<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Destilación de IA - Informe Técnico</title>
    <style>
        /* --- ESTILOS GENERALES --- */
        :root {
            --primary-color: #2563eb; /* Azul moderno */
            --secondary-color: #1e293b; /* Gris oscuro */
            --bg-color: #f3f4f6;
            --container-bg: #ffffff;
            --text-color: #334155;
            --code-bg: #1e1e1e;
            --code-text: #d4d4d4;
            --accent: #3b82f6;
        }

        body {
            font-family: 'Segoe UI', Roboto, Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            background-color: var(--bg-color);
            margin: 0;
            padding: 20px;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            background-color: var(--container-bg);
            padding: 50px;
            border-radius: 12px;
            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);
        }

        /* --- TIPOGRAFÍA --- */
        h1, h2, h3 {
            color: var(--secondary-color);
            font-weight: 700;
            margin-top: 1.5em;
        }

        h1 {
            font-size: 2.5rem;
            border-bottom: 3px solid var(--primary-color);
            padding-bottom: 10px;
            margin-top: 0;
        }

        h2 {
            font-size: 1.8rem;
            border-bottom: 1px solid #e2e8f0;
            padding-bottom: 5px;
        }

        h3 {
            font-size: 1.4rem;
            color: var(--primary-color);
        }

        p {
            margin-bottom: 1.2em;
            text-align: justify;
        }

        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 600;
            transition: color 0.2s;
        }

        a:hover {
            color: #1d4ed8;
            text-decoration: underline;
        }

        /* --- PORTADA Y AUTOR --- */
        .header-section {
            text-align: center;
            margin-bottom: 60px;
            padding: 40px 0;
            background: linear-gradient(to right, #eff6ff, #fff);
            border-radius: 8px;
        }

        .author {
            font-size: 1.2rem;
            color: #64748b;
            margin-top: 10px;
            font-style: italic;
        }

        .author strong {
            color: var(--secondary-color);
            font-style: normal;
        }

        /* --- ÍNDICE --- */
        .toc {
            background-color: #f8fafc;
            padding: 25px;
            border-radius: 8px;
            border-left: 5px solid var(--primary-color);
            margin-bottom: 40px;
        }

        .toc h2 {
            margin-top: 0;
            border: none;
            font-size: 1.5rem;
        }

        .toc ul {
            list-style-type: none;
            padding-left: 0;
        }

        .toc li {
            margin-bottom: 8px;
        }

        .toc li::before {
            content: "•";
            color: var(--primary-color);
            font-weight: bold;
            display: inline-block;
            width: 1em;
            margin-left: -1em;
        }
        
        .toc a {
            color: var(--text-color);
        }
        .toc a:hover {
            color: var(--primary-color);
        }

        /* --- CÓDIGO --- */
        pre {
            background-color: var(--code-bg);
            color: var(--code-text);
            padding: 20px;
            border-radius: 8px;
            overflow-x: auto;
            font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
            font-size: 0.9rem;
            border: 1px solid #333;
        }

        code {
            font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
        }

        p code {
            background-color: #e2e8f0;
            color: #d63384; /* Rosa para resaltar inline code */
            padding: 2px 5px;
            border-radius: 4px;
            font-size: 0.9em;
        }

        /* --- IMÁGENES --- */
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 30px auto;
            border-radius: 8px;
            box-shadow: 0 10px 15px -3px rgba(0, 0, 0, 0.1);
        }

        /* --- TABLAS --- */
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 25px 0;
            font-size: 1rem;
            font-family: sans-serif;
            min-width: 400px;
            box-shadow: 0 0 20px rgba(0, 0, 0, 0.05);
        }

        table thead tr {
            background-color: var(--primary-color);
            color: #ffffff;
            text-align: left;
        }

        table th, table td {
            padding: 12px 15px;
        }

        table tbody tr {
            border-bottom: 1px solid #dddddd;
        }

        table tbody tr:nth-of-type(even) {
            background-color: #f3f3f3;
        }

        table tbody tr:last-of-type {
            border-bottom: 2px solid var(--primary-color);
        }

        /* --- LISTAS --- */
        ul.custom-list {
            list-style: none;
            padding-left: 20px;
        }
        ul.custom-list li {
            margin-bottom: 10px;
            position: relative;
        }
        ul.custom-list li::before {
            content: "✓";
            color: var(--primary-color);
            font-weight: bold;
            position: absolute;
            left: -20px;
        }

        /* --- PREGUNTAS Y RESPUESTAS --- */
        .faq-item {
            background-color: #fff;
            border: 1px solid #e2e8f0;
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 15px;
        }
        .faq-question {
            font-weight: 700;
            color: var(--secondary-color);
            margin-bottom: 10px;
            display: block;
        }
        .faq-answer {
            color: var(--text-color);
        }

    </style>
</head>
<body>

<div class="container">

    <div class="header-section">
        <h1>DESTILACIÓN DE IA</h1>
        <p class="author">Realizado por: <strong>Ismael Herrero de la Torre</strong></p>
        <img src="images/image4.png" alt="Ilustración Destilación IA" style="max-width: 600px;">
    </div>

    <div class="toc" id="indice">
        <h2>Índice</h2>
        <ul>
            <li><a href="#intro">Introducción</a></li>
            <li><a href="#que-es">¿Qué es la destilación de modelos?</a></li>
            <li><a href="#modelos">¿Qué modelos usé?</a></li>
            <li><a href="#entrenamiento">Cómo entrené el modelo destilado</a></li>
            <li><a href="#diferencias">Diferencias entre el profesor y el estudiante</a></li>
            <li><a href="#por-que-tonto">¿Por qué el modelo destilado es más "tonto"?</a></li>
            <li><a href="#lm-studio">Cargar modelo en LM Studio (GGUF)</a></li>
            <li><a href="#metricas">Medición (Divergencia KL y Perplexity)</a></li>
            <li><a href="#acertijos">Batería de preguntas (Acertijos)</a></li>
            <li><a href="#graficas">Gráficas de resultados</a></li>
            <li><a href="#conclusion">Conclusión</a></li>
        </ul>
    </div>

    <h2 id="intro">Introducción</h2>
    <p>En los últimos años, los modelos de lenguaje de gran tamaño han demostrado un rendimiento sobresaliente en tareas de comprensión, generación y razonamiento. Sin embargo, su elevado coste computacional dificulta su uso en entornos con recursos limitados o en aplicaciones que requieren respuestas rápidas. En este contexto surge la destilación de modelos (<em>knowledge distillation</em>), una técnica que permite transferir el comportamiento de un modelo grande (teacher) a uno más pequeño (student), reduciendo su tamaño y acelerando su ejecución sin necesidad de reentrenar desde cero.</p>
    
    <p>El objetivo de este trabajo es analizar de forma práctica el proceso de destilación y evaluar su impacto tanto en la precisión como en la velocidad de generación. Para ello se han comparado dos modelos: un modelo base de mayor capacidad (GPT-Medium) y un modelo reducido obtenido mediante destilación (GPT-Small). La evaluación se ha realizado mediante un conjunto de preguntas técnicas relacionadas con el aprendizaje automático, así como mediante la medición de <em>tokens por segundo</em> como indicador de eficiencia.</p>
    
    <p>A lo largo del proyecto se han generado gráficas comparativas que permiten visualizar de manera clara las diferencias entre ambos modelos. Estas representaciones facilitan la interpretación de los resultados y permiten valorar hasta qué punto la destilación consigue un equilibrio adecuado entre rendimiento y eficiencia. El trabajo concluye con un análisis crítico de los resultados obtenidos y una reflexión sobre la utilidad real de la destilación en escenarios prácticos.</p>

    <h2 id="que-es">¿Qué es la destilación de modelos?</h2>
    <p>La destilación de modelos (<em>knowledge distillation</em>) es una técnica utilizada en aprendizaje automático para transferir el conocimiento de un modelo grande y preciso (<strong>teacher</strong>) a un modelo más pequeño y eficiente (<strong>student</strong>).</p>
    <p>El proceso consiste en entrenar el modelo pequeño para que imite las salidas del modelo grande, no solo las etiquetas reales. De esta forma, el student aprende patrones, relaciones y comportamientos que el teacher ya ha capturado, pero con un coste computacional mucho menor.</p>
    
    <p><strong>La destilación permite:</strong></p>
    <ul class="custom-list">
        <li>Reducir el tamaño del modelo.</li>
        <li>Acelerar la generación de texto.</li>
        <li>Disminuir el consumo de memoria.</li>
        <li>Mantener un rendimiento razonablemente cercano al modelo original.</li>
    </ul>
    <p>Es una técnica muy utilizada cuando se necesita desplegar modelos en dispositivos con recursos limitados o cuando la velocidad es un factor crítico.</p>

    <h2 id="modelos">¿Qué modelos usé?</h2>
    <p>En este trabajo se utilizaron dos modelos de lenguaje con distinta capacidad y propósito:</p>

    <h3>1. GPT-Medium (Teacher)</h3>
    <p>Es el modelo de mayor tamaño y precisión. Se utiliza como referencia porque ofrece respuestas más completas y coherentes. Su función en el proyecto es actuar como modelo maestro del cual se extrae el conocimiento.</p>
    <ul class="custom-list">
        <li>Mayor número de parámetros.</li>
        <li>Mayor precisión en tareas técnicas.</li>
        <li>Menor velocidad de generación.</li>
    </ul>

    <h3>2. GPT-Small (Student destilado)</h3>
    <p>Es el modelo reducido obtenido mediante destilación. Su objetivo es imitar el comportamiento del teacher, pero con un coste computacional mucho menor.</p>
    <ul class="custom-list">
        <li>Menos parámetros.</li>
        <li>Menor precisión.</li>
        <li>Mucha mayor velocidad de generación.</li>
        <li>Ideal para entornos donde la eficiencia es prioritaria.</li>
    </ul>

    <h2 id="entrenamiento">Cómo entrené el modelo destilado</h2>
    <h3>Pasos Previos</h3>
    <p>Empezamos instalando las librerías habiendo descargado previamente Python.</p>
    <img src="images/image15.png" alt="Instalación de librerías" style="max-width: 500px;">
    
    <p>Actualización de pip:</p>
    <img src="images/image11.png" alt="Update PIP">

    <h3>El código de destilación</h3>
    <p>Creé el archivo <code>destilacion.py</code>. Tras varios ajustes por incompatibilidades con versiones nuevas de Python, el script final y funcional es el siguiente:</p>

    <pre><code class="language-python">from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset
import torch
import torch.nn.functional as F
from transformers import Trainer, TrainingArguments

# -----------------------------
# 1. Cargar modelos
# -----------------------------
teacher = AutoModelForCausalLM.from_pretrained("gpt2-medium")
student = AutoModelForCausalLM.from_pretrained("gpt2")
tokenizer = AutoTokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token  # IMPORTANTE

# -----------------------------
# 2. Cargar dataset limpio (AG News)
# -----------------------------
dataset = load_dataset("ag_news")
# Reducir dataset a 5000 ejemplos (MUY IMPORTANTE PARA CPU)
dataset["train"] = dataset["train"].select(range(5000))

# -----------------------------
# 3. Preparar texto
# -----------------------------
def preparar(batch):
    texto = batch["text"]
    if not isinstance(texto, str):
        texto = str(texto)
    texto = texto.strip()
    if texto == "":
        texto = " "
    return {"text": texto}

dataset = dataset.map(preparar)

# -----------------------------
# 4. Tokenizar dataset
# -----------------------------
def tokenize(batch):
    tokens = tokenizer(
        batch["text"],
        truncation=True,
        padding="max_length",
        max_length=128
    )
    if "labels" in tokens:
        del tokens["labels"]
    return tokens

tokenized = dataset.map(tokenize, batched=True)

# -----------------------------
# 5. Trainer personalizado para distilación
# -----------------------------
class DistillationTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        if "labels" in inputs:
            del inputs["labels"]

        outputs_student = model(**inputs)
        logits_student = outputs_student.logits

        with torch.no_grad():
            outputs_teacher = teacher(**inputs)
            logits_teacher = outputs_teacher.logits

        # Cálculo de la pérdida usando divergencia KL
        loss = F.kl_div(
            logits_student.log_softmax(dim=-1),
            logits_teacher.softmax(dim=-1),
            reduction="batchmean"
        )

        return (loss, outputs_student) if return_outputs else loss

# -----------------------------
# 6. Configuración del entrenamiento
# -----------------------------
training_args = TrainingArguments(
    output_dir="./destilado",
    per_device_train_batch_size=2,
    num_train_epochs=1,
    logging_steps=50,
)
trainer = DistillationTrainer(
    model=student,
    args=training_args,
    train_dataset=tokenized["train"],
)

# -----------------------------
# 7. Entrenar
# -----------------------------
trainer.train()

# -----------------------------
# 8. Guardar modelo destilado
# -----------------------------
student.save_pretrained("mi_modelo_destilado")
tokenizer.save_pretrained("mi_modelo_destilado")</code></pre>

    <p>Ejecutamos el proceso. Se redujo el dataset a 5,000 ejemplos para evitar tiempos de cómputo excesivos en CPU.</p>
    <img src="images/image8.png" alt="Proceso de entrenamiento">
    <img src="images/image10.png" alt="Fin del entrenamiento">

    <h2 id="diferencias">Diferencias entre el profesor y el estudiante destilado</h2>
    <p>La diferencia fundamental entre ambos modelos está en su capacidad, tamaño y objetivo dentro del proceso de destilación. Aunque comparten arquitectura, no juegan el mismo papel.</p>

    <table>
        <thead>
            <tr>
                <th>Característica</th>
                <th>Teacher (GPT-Medium)</th>
                <th>Student (GPT-Small Destilado)</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td><strong>Parámetros</strong></td>
                <td>355M (Mayor capacidad)</td>
                <td>124M (Menor complejidad)</td>
            </tr>
            <tr>
                <td><strong>Precisión</strong></td>
                <td>Alta. Respuestas completas y coherentes.</td>
                <td>Inferior. No captura todos los patrones.</td>
            </tr>
            <tr>
                <td><strong>Velocidad</strong></td>
                <td>Lento (más recursos).</td>
                <td>Muy rápido (más tokens/seg).</td>
            </tr>
            <tr>
                <td><strong>Función</strong></td>
                <td>Fuente de conocimiento.</td>
                <td>Aproximación eficiente.</td>
            </tr>
        </tbody>
    </table>

    <h2 id="por-que-tonto">¿Por qué el modelo destilado es más tonto?</h2>
    <p>Esto ocurre por varias razones:</p>
    <ol>
        <li><strong>Menos parámetros = menos capacidad de aprendizaje:</strong> Un modelo grande puede almacenar patrones, relaciones y matices que uno pequeño simplemente no puede representar. Es como comparar un libro de 500 páginas (teacher) con un resumen de 80 páginas (student).</li>
        <li><strong>La destilación transmite comportamiento, no inteligencia completa:</strong> El student aprende a imitar las respuestas del teacher, pero no sus razonamientos internos profundos.</li>
        <li><strong>Generalización:</strong> Con menos capacidad, el modelo pequeño tiende a simplificar demasiado o cometer errores conceptuales.</li>
        <li><strong>Prioridad velocidad:</strong> El objetivo del student no es ser perfecto, sino ser rápido y eficiente.</li>
    </ol>

    <h2 id="lm-studio">Cómo cargaríamos mi modelo destilado en LM Studio?</h2>
    <h3>Convertir el modelo HuggingFace &rarr; GGUF</h3>
    <p>Para cargar el modelo en herramientas locales como LM Studio, es necesario convertirlo a formato GGUF. Utilicé la herramienta <code>llama.cpp</code>.</p>
    
    <p>Cloné el repositorio de GitHub:</p>
    <img src="images/image13.png" alt="Github Llama.cpp">
    
    <p>Ejecuté el script de conversión:</p>
    <pre><code class="language-bash">python .\convert_hf_to_gguf.py "C:\Users\Izan\mi_modelo_destilado" --outfile "C:\Users\Izan\mi_modelo_destilado.gguf"</code></pre>
    
    <img src="images/image3.png" alt="Consola conversión">
    
    <p>Finalmente, cargamos el archivo <code>.gguf</code> en LM Studio:</p>
    <img src="images/image7.png" alt="LM Studio Interface" style="max-width: 400px;">

    <h2 id="metricas">Medición (KL Divergence y Perplexity)</h2>
    <p>Utilicé un script para calcular qué tan similar es el estudiante al maestro.</p>
    
    <h3>Divergencia KL (Kullback-Leibler)</h3>
    <p>Mide la distancia entre las distribuciones de probabilidad del teacher y el student. Cuanto mayor es la KL, más diferente es el comportamiento.</p>
    <ul>
        <li><strong>Resultado obtenido:</strong> 24.98</li>
    </ul>
    <p>Esto indica que el modelo pequeño se desvía significativamente del comportamiento del modelo grande, lo cual es esperado dado el recorte drástico de parámetros.</p>

    <h3>Perplexity (Perplejidad)</h3>
    <p>Indica qué tan "sorprendido" está el modelo al ver nuevos datos. Cuanto más baja, mejor predice el siguiente token.</p>
    <ul>
        <li><strong>Resultado obtenido:</strong> 62.1</li>
    </ul>

    <table>
        <thead>
            <tr>
                <th>Perplexity</th>
                <th>Interpretación</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>20 – 40</td>
                <td>Modelo competente</td>
            </tr>
            <tr>
                <td>40 – 80</td>
                <td>Modelo limitado (Aquí cae nuestro student)</td>
            </tr>
            <tr>
                <td>80 – 150</td>
                <td>Modelo flojo</td>
            </tr>
            <tr>
                <td>> 150</td>
                <td>Modelo ineficaz</td>
            </tr>
        </tbody>
    </table>

    <h2 id="acertijos">Pila de acertijos que usé</h2>
    <p>Realicé 10 preguntas técnicas para evaluar cualitativamente los modelos. Aquí algunos ejemplos de las respuestas esperadas (Knowledge):</p>

    <div class="faq-item">
        <span class="faq-question">1. ¿Qué es la entropía cruzada (cross-entropy) en machine learning?</span>
        <span class="faq-answer">Es una función de pérdida que mide la diferencia entre la distribución real de las etiquetas y la distribución predicha por el modelo. Se usa mucho en clasificación.</span>
    </div>

    <div class="faq-item">
        <span class="faq-question">2. ¿Qué diferencia hay entre modelo autoregresivo y enmascarado?</span>
        <span class="faq-answer">Autoregresivo predice la siguiente palabra (GPT). Enmascarado predice palabras ocultas en el medio (BERT).</span>
    </div>

    <div class="faq-item">
        <span class="faq-question">3. ¿Qué es el gradiente en el descenso de gradiente?</span>
        <span class="faq-answer">Vector de derivadas parciales que indica la dirección en la que la función de pérdida aumenta. El descenso va en dirección opuesta.</span>
    </div>
    
    <div class="faq-item">
        <span class="faq-question">4. ¿Para qué sirve softmax?</span>
        <span class="faq-answer">Convierte un vector de valores en una distribución de probabilidad (suman 1).</span>
    </div>

    <p><em>(El resto de preguntas abarcaban embeddings, overfitting, attention mechanism, validación, hiperparámetros, etc.)</em></p>

    <h2 id="graficas">Gráficas</h2>
    
    <h3>Medición de la calidad de respuestas</h3>
    <p>Comparativa de aciertos sobre las 10 preguntas técnicas planteadas.</p>
    <img src="images/image14.png" alt="Gráfica Calidad">

    <h3>Medición de la velocidad de respuestas</h3>
    <p>Tokens por segundo generados.</p>
    <img src="images/image12.png" alt="Gráfica Velocidad">

    <h2 id="conclusion">Conclusión</h2>
    <p>A lo largo de este proyecto he desarrollado un proceso completo de destilación de modelos de lenguaje, evaluando tanto su rendimiento como su eficiencia computacional. El objetivo principal era comprobar si un modelo reducido (<strong>GPT-Small</strong>, student) podía aproximarse al comportamiento de un modelo mayor (<strong>GPT-Medium</strong>, teacher) manteniendo una mejora significativa en velocidad y consumo de recursos.</p>

    <p>En primer lugar, los resultados cualitativos mostraron una diferencia clara: el modelo teacher alcanzó <strong>8 aciertos sobre 10</strong>, mientras que el modelo student obtuvo <strong>3</strong>. Esta diferencia es coherente con la literatura sobre destilación, donde se espera una pérdida moderada de precisión a cambio de una mayor eficiencia.</p>

    <p>En segundo lugar, al analizar la velocidad, el modelo destilado mostró su principal ventaja: <strong>generó texto más del doble de rápido</strong> que el modelo original. Esta mejora confirma que la destilación no solo reduce el tamaño del modelo, sino que también optimiza su capacidad de respuesta.</p>

    <p>En conjunto, el trabajo demuestra que la destilación es una técnica eficaz para obtener modelos más ligeros y rápidos. Aunque el modelo student no iguala al teacher en precisión, sí ofrece una mejora sustancial en eficiencia, validando su uso para entornos donde la velocidad es crítica.</p>

</div>

</body>
</html>